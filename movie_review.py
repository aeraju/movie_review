# -*- coding: utf-8 -*-
"""Movie_Review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DF3f707BmNkFyks4AqohsFyQ1k2sifev
"""

#Importing Libraries 
import pandas as pd
import matplotlib.pyplot as plt
import re
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.preprocessing.sequence import pad_sequences
from keras.layers.core import Dense,Activation,Dropout
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding, Bidirectional,Flatten
from keras.utils import to_categorical
#from google.colab import files

#Importing data
trainData_url= 'https://raw.githubusercontent.com/aeraju/movie_review/master/train.tsv'
testData_url = 'https://raw.githubusercontent.com/aeraju/movie_review/master/test.tsv'
SampleSubmissionUrl ='https://raw.githubusercontent.com/aeraju/movie_review/master/sampleSubmission.csv'
trainTable = pd.read_csv(trainData_url,sep='\t')
testTable = pd.read_csv(testData_url,sep='\t')
sampleTable = pd.read_csv(SampleSubmissionUrl ,sep=',')

#Initializing tokenizer
t = Tokenizer()

#preprocessing data
def preProcess(Phrase):
  proData = Phrase.str.lower()
  proData = Phrase.map(lambda x: re.sub('[^a-zA-z0-9\s]','',x))
  return proData

def sequenceAndPadding(data,length):
  data = t.texts_to_sequences(data)
  data = pad_sequences(data,length)
  return data



#Distribution of movie reviews with sentiments
'''
count=sentDist = trainTable['Sentiment'].value_counts().sort_index() #sentiment Distribution
sentDist=sentDist.plot.bar(title ='sentiment distribution')
sentDist.set_xticklabels(('negative', 'somewhat negative', 'neutral', 'somewhat positive', 'positive'))
sentDist.set_xlabel('Sentiment')
sentDist.set_ylabel('counts')
for i,j in enumerate(count):
  sentDist.text(i-0.25,j+450,str(j))
'''

#Distribution of word count
trainTable['proData'] = preProcess(trainTable['Phrase'])
wordCount = trainTable['proData'].apply(lambda x:len(x.split()))
count= wordCount= wordCount.value_counts().sort_index()
max_index = max(count.index)
'''
wordCount = wordCount.plot.bar(figsize=[28,5],title ='wordCount')
wordCount.set_xlabel('word length')
for i,j in enumerate(count):
  wordCount.text(i-0.25,j+500,str(j))
'''
# Preprocessing Train Data
xtrain = preProcess(trainTable['Phrase'])
ytrain = trainTable['Sentiment']
ytrain = [to_categorical(i,num_classes=5)for i in ytrain]
t.fit_on_texts(xtrain)
xtrain = sequenceAndPadding(xtrain,max_index)

# Model
embedInDim = len(t.word_index)+1
embedOutDim = 50
model = Sequential()
model.add(Embedding(embedInDim,embedOutDim,input_length=max_index))
model.add(Bidirectional(LSTM(units=32,recurrent_dropout=0.1, return_sequences=True)))
model.add(Flatten())
model.add(Dense(5,activation='softmax'))
model.compile(loss = 'categorical_crossentropy',optimizer='adam', metrics = ['accuracy'])
print(model.summary())



#training
model.fit(xtrain,np.array(ytrain),batch_size=64,epochs=10,verbose=1,validation_split=0)

#Predicting the output
xtest = preProcess(testTable['Phrase'])
xtest = sequenceAndPadding(xtest,max_index)

#prediction
prediction = model.predict_classes(xtest)

#saving the file
sampleTable.Sentiment = prediction 
sampleTable.to_csv(r'results.csv', sep=',', index=False)
#files.download('results.csv')

